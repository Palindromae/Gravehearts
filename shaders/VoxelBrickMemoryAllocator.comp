#version 460
#extension GL_EXT_nonuniform_qualifier : enable
#extension GL_ARB_shading_language_include  : enable
#extension GL_EXT_shader_explicit_arithmetic_types_int64 : require
#extension GL_EXT_shader_atomic_int64 : require

#include "ChunkGPUConst.h"
#include "VoxelBrick.h"
#include "ChunkGenerationConst.h"

#include "Helpers/ChunkHeaders.glsl"
#include "Helpers/GPUMemory.glsl"


layout(set = 0, binding = 0)  buffer  ChunkHeadersN{
    ChunkHeader[] ChunkHeaders;
};


layout(set = 1, binding = 0)  buffer  GPUMemoryN{
    VoxelBrick[] GPUMemory;
};


layout(set = 2, binding = 0)  buffer  GPUVoxelMutexesN{
    int[] GPUVoxelMutexes;
};


layout(set = 3, binding = 0)  buffer  fromN{
    UnexpandedNode[] fromNodes;
};

layout(set = 4, binding = 0)  buffer  toN{
    UnexpandedNode[] toNodes;
};

layout(set = 5, binding = 0)  buffer  heightmapN{
    float[] heightmap_world;
};


#include "Helpers/ChunkIndexing.glsl"


layout (local_size_x = 1, local_size_y = 1, local_size_z = 1) in;

layout (push_constant) uniform constant
{
    ivec3 ChunkPosition;
    int quality;
    int volumeID;
};

struct NodeExpansion{
    uint blockValue;
    int shouldKeepGoing;
};

//--#Insert ChunkHeaderAlloc.glsl
//#Insert VoxelBrickMemAlloc.glsl

NodeExpansion ExpandNode(vec3 position, vec3 chunkposition, int quality)
{
    float heightmap_value = 5;
    float size = QualityToSize(quality);
   
    NodeExpansion expansion;
    expansion.blockValue = 0;
    expansion.shouldKeepGoing = 0;

    int noiseMeasures = int(min(size / heightmap_noiseSize, 1));
    float minSize = QualityToSizePtr(MipsUnderOne);

    if (quality == 0)
        return expansion;

    // Heightmap verification 
    for (int x = 0; x <= noiseMeasures; x++)
    {
        for(int y = 0; y <= noiseMeasures; y++)
        {
            
            // retrieve value
            heightmap_value = heightmap_world[ChunkPositionToHeightmapIndex(position,ivec2(x,y))];

            expansion.blockValue = uint(position.y<heightmap_value);
            // size - minSize makes sure that the highest smallest block is being referenced
            bool isAccepted = (position.y + size - minSize) > heightmap_value;
            isAccepted = isAccepted && (heightmap_value>(0 + position.y));
            expansion.shouldKeepGoing |= int(isAccepted);
        }
    }

    return expansion;
}

uint64_t SetBitForMask(int index, uint blockValue){
    return uint64_t(blockValue>0) <<index;
}

void main()
{
    int nodeID = int(gl_GlobalInvocationID.x)/64;
    int local_id = int(gl_GlobalInvocationID.x) & 63; // fast modulo

    int count = fromNodes[0].position * 64;
    bool isHeader = MaxQuality == quality;
    if (!(isHeader && gl_GlobalInvocationID.x == 0))
    {
        if (gl_GlobalInvocationID.x >= count)
            return;
    }
  
    UnexpandedNode parent_node = fromNodes[nodeID+1];
    float size = QualityToSize(quality);

    vec3 pos;

    int headerPtrIndex;

    if (isHeader){
        headerPtrIndex = GetChunkVolumeIndexFromPos(ChunkHeaders[volumeID].position, ChunkPosition);
        pos = vec3(0);
    } else {
        pos = decode_pos(parent_node.position) + decode_local_ID(local_id, size);

    }
    
    NodeExpansion expansion = ExpandNode(pos, ChunkPosition, quality);
   
   int leaf = (1-min(quality,1)) * 1;

    if(expansion.shouldKeepGoing == 0){
        // This node should be set to a constant block value
        
        if (isHeader){
            // If it is at the max quality, this is the root, set the header.
            ChunkHeaders[volumeID].ptrs[headerPtrIndex] = const_value + expansion.blockValue & ptr_mask;
        } else {
            // is a leaf and constant
            GPUMemory[parent_node.mempos].ptrs[local_id] = const_value + expansion.blockValue; // 32 bit is 1 //  + leaf No marking leaves
            atomicOr(GPUMemory[parent_node.mempos].mask, SetBitForMask(local_id, expansion.blockValue));
        }
        
        return;
    }

  

    if(leaf == 1)
    {
        // this is a leaf
        GPUMemory[parent_node.mempos].ptrs[local_id] = const_value + expansion.blockValue; // We dont mark leaves right now + leaf
        atomicOr(GPUMemory[parent_node.mempos].mask, SetBitForMask(local_id, expansion.blockValue));
        return;
    }

    // This node needs to be expanded

    int index = atomicAdd(toNodes[0].position, 1) + 1;
    float next_size = QualityToSize(quality-1);
  
    // Fetch new memory
    uint chapter_hash = positionalHash(ChunkHeaders[volumeID].position);
    chapter_hash = CombinePositionalAndIDHash(gl_GlobalInvocationID.x, chapter_hash);
    chapter_hash = getChapterID(chapter_hash);

   // Hash data to try to spread the memory accesses evenly
    uint memindex = attemptToObtainChapterPage(chapter_hash);


    // Set the data 
    if (isHeader){
      ChunkHeaders[volumeID].ptrs[headerPtrIndex] = memindex & ptr_mask;
    } else {
      GPUMemory[parent_node.mempos].ptrs[local_id] = memindex & ptr_mask; // 30^2-1 32nd bit is 0
      GPUMemory[parent_node.mempos].blockID = expansion.blockValue; // set the block ID of the node for dynamic LOD
      atomicOr(GPUMemory[parent_node.mempos].mask, 1UL<<local_id); //SetBitForMask(local_id, expansion.blockValue) Only reason to decend is if there is a block

    }

    // Create base node to be expanded in the next cycle
    UnexpandedNode newNode;
    newNode.mempos = memindex;
    newNode.position = encode_pos(pos);
    toNodes[index] = newNode;

}
